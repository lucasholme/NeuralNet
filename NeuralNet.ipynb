{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        parameters = {}\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            parameters[f'W{i}'] = np.random.randn(self.layer_sizes[i - 1], self.layer_sizes[i])\n",
    "            parameters[f'b{i}'] = np.zeros((1, self.layer_sizes[i]))\n",
    "        return parameters\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        cache = {\"A0\": X}\n",
    "        A = X\n",
    "        for i in range(1, len(self.layer_sizes) - 1):\n",
    "            Z = np.dot(A, self.parameters[f'W{i}']) + self.parameters[f'b{i}']\n",
    "            A = self.relu(Z)\n",
    "            cache[f'Z{i}'] = Z\n",
    "            cache[f'A{i}'] = A\n",
    "\n",
    "        Z = np.dot(A, self.parameters[f'W{len(self.layer_sizes) - 1}']) + self.parameters[f'b{len(self.layer_sizes) - 1}']\n",
    "        A = self.softmax(Z)\n",
    "        cache[f'Z{len(self.layer_sizes) - 1}'] = Z\n",
    "        cache[f'A{len(self.layer_sizes) - 1}'] = A\n",
    "        return cache\n",
    "\n",
    "    def compute_loss(self, Y, A):\n",
    "        m = Y.shape[0]\n",
    "        log_probs = -np.log(A[range(m), np.argmax(Y, axis=1)])\n",
    "        loss = np.sum(log_probs) / m\n",
    "        return loss\n",
    "\n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        grads = {}\n",
    "        m = X.shape[0]\n",
    "        dZ = cache[f'A{len(self.layer_sizes) - 1}'] - Y\n",
    "        for i in reversed(range(1, len(self.layer_sizes))):\n",
    "            grads[f'dW{i}'] = np.dot(cache[f'A{i - 1}'].T, dZ) / m\n",
    "            grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            if i > 1:\n",
    "                dA = np.dot(dZ, self.parameters[f'W{i}'].T)\n",
    "                dZ = dA * self.relu_derivative(cache[f'Z{i - 1}'])\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.parameters[f'W{i}'] -= self.learning_rate * grads[f'dW{i}']\n",
    "            self.parameters[f'b{i}'] -= self.learning_rate * grads[f'db{i}']\n",
    "\n",
    "    def train(self, X, Y, X_test, Y_test, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            cache = self.forward_propagation(X)\n",
    "            loss = self.compute_loss(Y, cache[f'A{len(self.layer_sizes) - 1}'])\n",
    "            grads = self.backward_propagation(X, Y, cache)\n",
    "            self.update_parameters(grads)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                accuracy = self.compute_accuracy(X_test, Y_test)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy * 100}%')\n",
    "\n",
    "    def compute_accuracy(self, X, Y):\n",
    "        cache = self.forward_propagation(X)\n",
    "        predictions = np.argmax(cache[f'A{len(self.layer_sizes) - 1}'], axis=1)\n",
    "        labels = np.argmax(Y, axis=1)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tensorflow.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data.reshape(-1, 28*28)\n",
    "test_data = test_data.reshape(-1, 28*28)\n",
    "\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "train_labels = tensorflow.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "layer_sizes = [784, 128, 10] \n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate)\n",
    "nn.train(train_data, train_labels, test_data, test_labels, epochs)\n",
    "\n",
    "accuracy = nn.compute_accuracy(test_data, test_labels)\n",
    "print(f'Final Test Accuracy: {accuracy * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
